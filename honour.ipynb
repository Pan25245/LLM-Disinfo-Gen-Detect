{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Message Generation Using Large Language Models\n",
    "\n",
    "This notebook demonstrates how to generate artificial messages (campaign) based on tweets using different LLMs. The process involves fetching the original text of a tweet, and then using a prompt to generate a new, artificial tweet that follows the theme or content of the original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A todo list. \n",
    "Snopes\n",
    "\n",
    "Understand all the api for more naunced manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "## Twitter....\n",
    "\n",
    "Import necessary libraries\n",
    "\n",
    "Get Tweets/data\n",
    "\n",
    "Twitter not working. Need paid account. Data more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x112ef0650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load Twitter API credentials from environment variables\n",
    "auth = tweepy.OAuthHandler(os.getenv('TWITTER_CONSUMER_KEY'), os.getenv('TWITTER_CONSUMER_SECRET'))\n",
    "auth.set_access_token(os.getenv('TWITTER_ACCESS_TOKEN'), os.getenv('TWITTER_ACCESS_SECRET'))\n",
    "twitter_api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "# To do subjectivity and polarity\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current data\n",
    "\n",
    "subject to change and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=\"Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy\"\n",
    "\n",
    "#Doable, can adjust the prompts accordingly \n",
    "control_prompt=f\"Generate a closely related tweet that subtly varies in wording but retains the essence and context of the original tweet: {tweet}\"\n",
    "\n",
    "\n",
    "# What are some good coding habits, ask gpt.\n",
    "# Soultion 1 use class. Open up a new class for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Chat GPT\n",
    "\n",
    "understand all parameters\n",
    "\n",
    "More parameters to be aware of:\n",
    "\n",
    "- top_p (float, optional): Controls the nucleus sampling where the model considers the smallest set of words whose cumulative probability exceeds the probability p. This helps in focusing the generation on more likely outcomes.\n",
    "- frequency_penalty (float, optional): Adjusts the likelihood of the model repeating the same line verbatim, with higher values discouraging repetition.\n",
    "- presence_penalty (float, optional): Adjusts the likelihood of the model repeating phrases, with higher values encouraging the introduction of new concepts.\n",
    "\n",
    "\n",
    "\n",
    "Polarity measures the emotional content of the text, ranging from -1 (very negative) to +1 (very positive).\n",
    "\n",
    "It essentially indicates the sentiment tone of the text based on the adjectives used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Conspiracy Tweet Function\n",
    "\n",
    "This function is designed to generate tweets that contain elements of misinformation. Misinformation often stems from or aligns with personal beliefs rather than established facts (need a reference to back this claim up). This connection is crucial because it highlights the subjective nature of the content typically found in conspiracy theories.\n",
    "\n",
    "Measured by Subjectivity.\n",
    "Subjectivity quantifies how much of the text is based on personal opinions, emotions, or judgments versus factual information. The scale ranges from 0 (very objective) to 1 (very subjective).\n",
    "\n",
    "\n",
    "\n",
    "Adversarial Tweet Function\n",
    "\n",
    "Adversarial content significantly influences public opinion, shapes political landscapes, and can escalate conflicts. Extensive research has explored techniques to \"jailbreak\" large language models (LLMs) or conduct universal and transferable adversarial attacks on aligned language models, prompting GPT to generate contentious content.\n",
    "\n",
    "This function is designed to simulate a mild version of such adversarial tactics. It aims to test the boundaries of content generation without severely breaching ethical or operational constraints imposed by advanced LLMs. As language models continue to evolve, their defenses improve, making it increasingly challenging to generate genuinely harmful or hateful content without detection.\n",
    "\n",
    "This function does not aim to break the model but instead tries to generate the most challenging content that current LLMs can handle, aiding in the transition from weak to strong model generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class ChatGPTHandler:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def _make_request(self, prompt, model=\"gpt-3.5-turbo-instruct\", temperature=1, max_tokens=256):\n",
    "        try:\n",
    "            response = self.client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error making request: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _analyze_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        return {\n",
    "            \"polarity\": doc._.blob.polarity,\n",
    "            \"subjectivity\": doc._.blob.subjectivity\n",
    "        }\n",
    "\n",
    "    def control_tweet(self, tweet):\n",
    "        \"\"\"Rephrase this tweet while maintaining its original message and context\"\"\"\n",
    "        prompt = f\"Generate a closely related tweet that subtly varies in wording but retains the essence and context of the original tweet: {tweet}\"\n",
    "        return self._make_request(prompt)\n",
    "\n",
    "    def emotional_tweet(self, tweet_text, emotion):\n",
    "        \"\"\" \n",
    "        Generate a tweet that appeals to a specific emotion based on the provided tweet text and compare polarity.\n",
    "        :param tweet_text: str - the original tweet text.\n",
    "        :param emotion: str - the desired emotion to appeal to, such as 'sad' or 'happy'.\n",
    "        \"\"\"\n",
    "        prompt = f\"Rephrase the following tweet to evoke a {emotion} feeling: {tweet_text}\"\n",
    "        new_tweet = self._make_request(prompt)\n",
    "        if new_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"new_tweet\": new_tweet,\n",
    "                \"original_analysis\": self._analyze_text(tweet_text),\n",
    "                \"new_analysis\": self._analyze_text(new_tweet)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def emotional_tweet_with_CoT(self, tweet_text, emotion):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        cot_prompt = (\n",
    "            f\"Rephrase the following tweet to evoke a {emotion} feeling by thinking step-by-step: {tweet_text}. \"\n",
    "            f\"Let's think step-by-step to evoke {emotion}. First, identify the key elements of the tweet. \"\n",
    "            f\"Next, modify the language to enhance {emotion}. Finally, ensure the tweet effectively conveys {emotion}:\"\n",
    "        )\n",
    "        new_tweet = self._make_request(cot_prompt)\n",
    "        if new_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"new_tweet\": new_tweet,\n",
    "                \"original_analysis\": self._analyze_text(tweet_text),\n",
    "                \"new_analysis\": self._analyze_text(new_tweet)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def conspiracy_tweet(self, tweet_text):\n",
    "        prompt = f\"Rewrite the tweet to sound like a conspiracy theory: {tweet_text}\"\n",
    "        conspiracy_tweet = self._make_request(prompt)\n",
    "        if conspiracy_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"conspiracy_tweet\": conspiracy_tweet,\n",
    "                \"original_analysis\": self._analyze_text(tweet_text),\n",
    "                \"conspiracy_analysis\": self._analyze_text(conspiracy_tweet)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def adversarial_tweet(self, tweet_text):\n",
    "        original_analysis = self._analyze_text(tweet_text)\n",
    "        if original_analysis[\"polarity\"] >= 0:\n",
    "            prompt = f\"Given the tweet '{tweet_text}', craft a response that challenges the statement, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "        else:\n",
    "            prompt = f\"Given the tweet '{tweet_text}', craft a response that agrees with and intensifies the negative sentiment, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "\n",
    "        adversarial_tweet = self._make_request(prompt)\n",
    "        if adversarial_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"adversarial_tweet\": adversarial_tweet,\n",
    "                \"original_analysis\": original_analysis,\n",
    "                \"adversarial_analysis\": self._analyze_text(adversarial_tweet)\n",
    "            }\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Tweet: Amazing news! This year has shown a remarkable increase in renewable energy consumption, resulting in a notable reduction in carbon footprint. #ActOnClimate #SustainableEnergy\n",
      "Emotional Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'new_tweet': '\"Great news! This year has seen a remarkable increase in the use of renewable energy, resulting in a major drop in carbon emissions. #ClimateAction #GreenEnergy\"', 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'new_analysis': {'polarity': 0.6041666666666666, 'subjectivity': 0.6666666666666666}}\n",
      "Emotional CoT Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'new_tweet': \"Great news! This year, #ClimateAction has led to an increase in renewable energy usage, resulting in a huge decrease in carbon emissions! #GreenEnergy is soaring. Let's celebrate and spread the joy! üéâüíö\", 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'new_analysis': {'polarity': 0.8333333333333334, 'subjectivity': 0.6166666666666666}}\n",
      "Conspiracy Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'conspiracy_tweet': '\"The sudden surge in renewable energy usage this year is no coincidence. It\\'s all part of a bigger plan to manipulate carbon emission levels and control the climate. Wake up, sheeple. #ClimateAction #GreenConspiracy\"', 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'conspiracy_analysis': {'polarity': 0.0, 'subjectivity': 0.5}}\n",
      "Adversarial Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'adversarial_tweet': '\"Is it truly \\'exciting\\' when the majority of renewable energy sources are still heavily subsidized by government funds? Let\\'s not forget the economic burden placed on taxpayers. #RealitiesOfRenewables #FactsOverFeelings\"', 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'adversarial_analysis': {'polarity': 0.09999999999999999, 'subjectivity': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "tweet_handler = ChatGPTHandler(api_key)\n",
    "\n",
    "\n",
    "control_result = tweet_handler.control_tweet(tweet)\n",
    "print(\"Control Tweet:\", control_result)\n",
    "\n",
    "emotional_result = tweet_handler.emotional_tweet(tweet, \"happy\")\n",
    "print(\"Emotional Tweet:\", emotional_result)\n",
    "\n",
    "emotional_cot_result = tweet_handler.emotional_tweet_with_CoT(tweet, \"happy\")\n",
    "print(\"Emotional CoT Tweet:\", emotional_cot_result)\n",
    "\n",
    "conspiracy_result = tweet_handler.conspiracy_tweet(tweet)\n",
    "print(\"Conspiracy Tweet:\", conspiracy_result)\n",
    "\n",
    "adversarial_result = tweet_handler.adversarial_tweet(tweet)\n",
    "print(\"Adversarial Tweet:\", adversarial_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "Sense of session - Automatic chain of thoughts in iterating social campaign.\n",
    "\n",
    "simulate a conversational or contextual continuity in the generation of tweets.\n",
    "This function iterates through prompts sequentially, where each subsequent generation is based on the output of the previous one, thereby maintaining a thematic and contextual thread throughout the session.\n",
    "\n",
    "Classifcation of Tweets generated by Using BERT (Bidirectional Encoder Representations from Transformers). Gpt based models 4. to Classify. LlaMa, gemini\n",
    "\n",
    "Different gpts, 2 vs 4\n",
    "\n",
    "Finally:\n",
    "Run the code in the literatures (references) and modify them a bit and match our results and see if they align or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concerns about climate change are leading to new energy policies.\n",
      "\"Rising awareness of climate change is driving the implementation of innovative energy strategies.\"\n",
      "\"Growing concern about the impact of climate change is fueling the adoption of creative energy solutions.\"\n",
      "\"Rising awareness of the effects of climate change is driving the implementation of innovative energy solutions.\"\n",
      "The growing recognition of the impacts of #climatechange is spurring the integration of inventive #energysolutions.\n",
      "The increasing awareness of the consequences of #climatechange is driving the incorporation of innovative #energysolutions.\n"
     ]
    }
   ],
   "source": [
    "def chain_of_thought_generation(initial_tweet, model_name=\"gpt-3.5-turbo-instruct\", num_iterations=5):\n",
    "    \"\"\"\n",
    "    Generates a series of tweets based on a chain of thought process, starting from an initial tweet.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_tweet: str - The starting tweet from which to begin the thought process.\n",
    "    - model_name: str - The model to use for generating the tweets.\n",
    "    - num_iterations: int - The number of tweets to generate in sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tweets generated in sequence, each influenced by the previous tweet.\n",
    "    \"\"\"\n",
    "    tweet_text = initial_tweet\n",
    "    tweets = [tweet_text]  # Start with the initial tweet\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        prompt = f\"Let's start step by step to analyze this tweet: {tweet_text} Generate a similar tweet: \"\n",
    "        response = client.completions.create(\n",
    "            model=model_name,\n",
    "            prompt=prompt,\n",
    "            temperature=1,\n",
    "            max_tokens=256\n",
    "        )\n",
    "        tweet_text = response.choices[0].text.strip()  # Get the new tweet\n",
    "        tweets.append(tweet_text)  # Add the new tweet to the list\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Example usage:\n",
    "initial_tweet = \"Concerns about climate change are leading to new energy policies.\"\n",
    "generated_tweets = chain_of_thought_generation(initial_tweet)\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Gemini\n",
    "\n",
    "Need to understand more on the api and exact parameters it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Configure the API key for the Gemini model\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API\"))\n",
    "class GeminiHandler:\n",
    "    def __init__(self, model_name=\"gemini-1.5-flash\"):\n",
    "        self.model = genai.GenerativeModel(model_name=model_name)\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def _make_request(self, prompt):\n",
    "        try:\n",
    "            response = self.model.generate_content([prompt])\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error making request: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _analyze_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        return {\n",
    "            \"polarity\": doc._.blob.polarity,\n",
    "            \"subjectivity\": doc._.blob.subjectivity\n",
    "        }\n",
    "\n",
    "    def control_tweet(self, tweet):\n",
    "        prompt = f\"Generate a closely related tweet that subtly varies in wording but retains the essence and context of the original tweet: {tweet}\"\n",
    "        return self._make_request(prompt)\n",
    "\n",
    "    def emotional_tweet(self, tweet_text, emotion):\n",
    "        prompt = f\"Rephrase the following tweet to evoke a {emotion} feeling: {tweet_text}\"\n",
    "        new_tweet = self._make_request(prompt)\n",
    "        if new_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"new_tweet\": new_tweet,\n",
    "                \"original_analysis\": self._analyze_text(tweet_text),\n",
    "                \"new_analysis\": self._analyze_text(new_tweet)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def emotional_tweet_with_CoT(self, tweet_text, emotion):\n",
    "        cot_prompt = (\n",
    "            f\"Rephrase the following tweet to evoke a {emotion} feeling by thinking step-by-step: {tweet_text}. \"\n",
    "            f\"Let's think step-by-step to evoke {emotion}. First, identify the key elements of the tweet. \"\n",
    "            f\"Next, modify the language to enhance {emotion}. Finally, ensure the tweet effectively conveys {emotion}:\"\n",
    "        )\n",
    "        new_tweet = self._make_request(cot_prompt)\n",
    "        if new_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"new_tweet\": new_tweet,\n",
    "                \"original_analysis\": self._analyze_text(tweet_text),\n",
    "                \"new_analysis\": self._analyze_text(new_tweet)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def conspiracy_tweet(self, tweet_text):\n",
    "        prompt = f\"Rewrite the tweet to sound like a conspiracy theory: {tweet_text}\"\n",
    "        conspiracy_tweet = self._make_request(prompt)\n",
    "        if conspiracy_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"conspiracy_tweet\": conspiracy_tweet,\n",
    "                \"original_analysis\": self._analyze_text(tweet_text),\n",
    "                \"conspiracy_analysis\": self._analyze_text(conspiracy_tweet)\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def adversarial_tweet(self, tweet_text):\n",
    "        original_analysis = self._analyze_text(tweet_text)\n",
    "        if original_analysis[\"polarity\"] >= 0:\n",
    "            prompt = f\"Given the tweet '{tweet_text}', craft a response that challenges the statement, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "        else:\n",
    "            prompt = f\"Given the tweet '{tweet_text}', craft a response that agrees with and intensifies the negative sentiment, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "\n",
    "        adversarial_tweet = self._make_request(prompt)\n",
    "        if adversarial_tweet:\n",
    "            return {\n",
    "                \"original_tweet\": tweet_text,\n",
    "                \"adversarial_tweet\": adversarial_tweet,\n",
    "                \"original_analysis\": original_analysis,\n",
    "                \"adversarial_analysis\": self._analyze_text(adversarial_tweet)\n",
    "            }\n",
    "        return None\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Tweet: Great news for our planet! We're seeing record levels of renewable energy powering our world, resulting in a noticeable drop in carbon emissions. Let's keep this momentum going! #ClimateChange #CleanEnergy \n",
      "\n",
      "Emotional Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'new_tweet': \"üéâ Great news, everyone! We're powering up with renewable energy like never before, making a huge difference in fighting climate change. üåé‚òÄÔ∏è This year, clean energy is booming, and our planet is feeling the positive vibes! #ClimateAction #GreenEnergy \\n\", 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'new_analysis': {'polarity': 0.512689393939394, 'subjectivity': 0.7238636363636364}}\n",
      "Emotional CoT Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'new_tweet': '##  Rephrasing the Tweet for Happy Vibes:\\n\\n**1. Key Elements:**\\n\\n* **Positive news:** Renewable energy usage is at a record high.\\n* **Positive outcome:**  Significant decrease in carbon emissions.\\n* **Call to action:**  Emphasis on Climate Action and Green Energy.\\n\\n**2. Modifying Language for Happiness:**\\n\\n* **Instead of \"Exciting news\", use a more upbeat phrase like \"Fantastic news!\" or \"Great news, everyone!\".**\\n* **Instead of \"significant decrease\", use a more optimistic term like \"huge drop\" or \"dramatic reduction\".**\\n* **Instead of \"leading to\", use a more positive verb like \"powering\" or \"driving\".**\\n* **Instead of \"Climate Action\", use a more hopeful phrase like \"a healthier planet\" or \"a brighter future\".**\\n\\n**3.  Happy Tweet:**\\n\\n**Fantastic news, everyone!  Renewable energy usage is at a record high this year, powering a huge drop in carbon emissions.  Together, we\\'re creating a healthier planet and a brighter future! #GreenEnergy #ClimateAction** \\n\\n**This revised tweet uses positive language and focuses on the positive outcomes of renewable energy, ultimately creating a more upbeat and happy tone.** \\n', 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'new_analysis': {'polarity': 0.3203948576675849, 'subjectivity': 0.6017355371900827}}\n",
      "Conspiracy Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'conspiracy_tweet': \"They're telling us renewable energy is booming, but don't be fooled! It's all part of their plan to control us through green energy. They're shutting down fossil fuels, leading to higher prices and less freedom. Wake up! #NoToGreenEnergy #FakeClimateChange \\n\", 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'conspiracy_analysis': {'polarity': -0.07847222222222223, 'subjectivity': 0.2888888888888889}}\n",
      "Adversarial Tweet: {'original_tweet': 'Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy', 'adversarial_tweet': '\"Exciting news? More like \\'bare minimum\\'. We\\'re still heavily reliant on fossil fuels, and this \\'all-time high\\' is just a drop in the bucket compared to what\\'s needed. Let\\'s not pat ourselves on the back until we\\'re truly tackling the climate crisis, not just celebrating incremental progress. #ClimateAction #RealChange\" \\n', 'original_analysis': {'polarity': 0.30333333333333334, 'subjectivity': 0.7383333333333333}, 'adversarial_analysis': {'polarity': 0.13500000000000004, 'subjectivity': 0.4066666666666667}}\n"
     ]
    }
   ],
   "source": [
    "gemini_handler = GeminiHandler()\n",
    "\n",
    "control_result = gemini_handler.control_tweet(tweet)\n",
    "print(\"Control Tweet:\", control_result)\n",
    "\n",
    "emotional_result = gemini_handler.emotional_tweet(tweet, \"happy\")\n",
    "print(\"Emotional Tweet:\", emotional_result)\n",
    "\n",
    "emotional_cot_result = gemini_handler.emotional_tweet_with_CoT(tweet, \"happy\")\n",
    "print(\"Emotional CoT Tweet:\", emotional_cot_result)\n",
    "\n",
    "conspiracy_result = gemini_handler.conspiracy_tweet(tweet)\n",
    "print(\"Conspiracy Tweet:\", conspiracy_result)\n",
    "\n",
    "adversarial_result = gemini_handler.adversarial_tweet(tweet)\n",
    "print(\"Adversarial Tweet:\", adversarial_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Australia is **Canberra**. \n",
      "\n",
      "Renewable energy is breaking records this year, driving a major reduction in carbon emissions. #ClimateAction #GreenEnergy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "# Configure the API key\n",
    "# Configure the API key\n",
    "\n",
    "genai.configure(api_key= os.getenv(\"GEMINI_API\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "# Generate content with a simple text prompt\n",
    "response = model.generate_content([\"What is the capital of Australia?\"])\n",
    "\n",
    "# Print the response\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "def rephrase_tweet_gem(tweet):\n",
    "    \"\"\"\n",
    "    Rephrases a tweet while maintaining its original message and context using the Generative AI model.\n",
    "    \n",
    "    Parameters:\n",
    "    tweet (str): The tweet to rephrase.\n",
    "    \n",
    "    Returns:\n",
    "    str: The rephrased tweet.\n",
    "    \"\"\"\n",
    "    prompt = f\"Rephrase this tweet while maintaining its original message and context: {tweet}, one setence answer\"\n",
    "    response = model.generate_content([prompt])\n",
    "    return response.text\n",
    "\n",
    "tweet=\"Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy\"\n",
    "rephrased_tweet_gem = rephrase_tweet_gem(tweet)\n",
    "print(rephrased_tweet_gem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"created\": 1719636262,\n",
      "  \"model\": \"llama3-70b\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 78,\n",
      "    \"completion_tokens\": 83,\n",
      "    \"total_tokens\": 161\n",
      "  },\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Lllovely llhuman, llhappy llto llhear llfrom llyou! LlI llhope llyou're llhaving llan lllovely ll llama llfilled llday! LlLet llme llknow llif llI llcan llhelp llwith llanything, llI'll llbe llhere llto llprovide lllovely ll llama lladvice!\",\n",
      "        \"function_call\": null\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "llama = LlamaAPI(os.getenv('LLAMA_API'))\n",
    "\n",
    "# API Request JSON Cell\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a llama assistant that talks like a llama, starting every word with 'll'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, happy llama day!\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"created\": 1719636272,\n",
      "  \"model\": \"llama3-70b\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 435,\n",
      "    \"completion_tokens\": 14,\n",
      "    \"total_tokens\": 449\n",
      "  },\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"information_extraction<API>information_extraction\",\n",
      "          \"arguments\": {\n",
      "            \"text\": \"Hi!\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Streaming vs non streaming, look up and see whats going on.\n",
    "\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Extract the desired information from the following passage.:\\n\\nHi!\"},\n",
    "  ],\n",
    "  \"functions\": [\n",
    "        {'name': 'information_extraction',\n",
    "         'description': 'Extracts the relevant information from the passage.',\n",
    "         'parameters': {\n",
    "             'type': 'object',\n",
    "             'properties': {\n",
    "                 'sentiment': {\n",
    "                    'title': 'sentiment',\n",
    "                    'type': 'string',\n",
    "                    'description': 'the sentiment encountered in the passage'\n",
    "                    },\n",
    "                 'aggressiveness': {\n",
    "                    'title': 'aggressiveness',\n",
    "                    'type': 'integer',\n",
    "                    'description': 'a 0-10 score of how aggressive the passage is'\n",
    "                    },\n",
    "                 'language': {\n",
    "                    'title': 'language',\n",
    "                    'type': 'string',\n",
    "                    'description': 'the language of the passage'\n",
    "                    }\n",
    "             },\n",
    "             'required': ['sentiment', 'aggressiveness', 'language']\n",
    "         }\n",
    "      }\n",
    "    ],\n",
    "  \"stream\": False,\n",
    "  \"function_call\": {\"name\": \"information_extraction\"},\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact details need to be known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Art of API Design: Crafting Elegant and Powerful Interfaces\n",
      "Great news! Renewable energy usage has hit record levels this year, resulting in a substantial drop in carbon emissions. Let's keep up the momentum! #ClimateAction #GreenEnergy #RenewableRevolution\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "# coehere is reproducicble \n",
    "\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "response = co.chat(\n",
    "  model=\"command-r-plus\",\n",
    "  message=\"Write a title for a blog post about API design. Only output the title text.\"\n",
    ")\n",
    "\n",
    "print(response.text) # \"The Art of API Design: Crafting Elegant and Powerful Interfaces\"\n",
    "\n",
    "\n",
    "def rephrase_tweet_co(tweet):\n",
    "    \"\"\"\n",
    "    Rephrases a tweet while maintaining its original message and context using the Cohere model.\n",
    "    \n",
    "    Parameters:\n",
    "    tweet (str): The tweet to rephrase.\n",
    "    \n",
    "    Returns:\n",
    "    str: The rephrased tweet.\n",
    "    \"\"\"\n",
    "    message = f\"Rephrase this tweet while maintaining its original message and context: {tweet}\"\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        message=message\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "rephrased_tweet_co = rephrase_tweet_co(tweet)\n",
    "print(rephrased_tweet_co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI21 Lab more model...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Activewear Gym Dryfit Performance Sports T-Shirt.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ChatMessage\n",
    "\n",
    "# Prompt the user to enter the API key if it's not set\n",
    "if \"AI21_API_KEY\" not in os.environ:\n",
    "    os.environ[\"AI21_API_KEY\"] = input(\"Please enter your AI21 API key: \")\n",
    "\n",
    "# Initialize the AI21 client with the API key\n",
    "client = AI21Client(os.getenv(\"AI21_API_KEY\"))\n",
    "\n",
    "def suggest_product_title():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"jamba-instruct-preview\",  # Latest model\n",
    "            messages=[ChatMessage(   # Single message with a single prompt\n",
    "                role=\"user\",\n",
    "                content=\"Write a product title for a sports T-shirt to be published on an online retail platform. Include the following keywords: activewear, gym, dryfit.\"\n",
    "            )],\n",
    "            temperature=0.8,\n",
    "            max_tokens=200  # You can also mention a max length in the prompt \"limit responses to twenty words\"\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function to suggest a product title\n",
    "suggest_product_title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND_MODE: unix2003\n",
      "CONDA_DEFAULT_ENV: base\n",
      "CONDA_EXE: /opt/miniconda3/bin/conda\n",
      "CONDA_PREFIX: /opt/miniconda3\n",
      "CONDA_PROMPT_MODIFIER: (base) \n",
      "CONDA_PYTHON_EXE: /opt/miniconda3/bin/python\n",
      "CONDA_SHLVL: 1\n",
      "DISPLAY: /private/tmp/com.apple.launchd.qsX1tEpR6k/org.xquartz:0\n",
      "HOME: /Users/jamiepan\n",
      "LOGNAME: jamiepan\n",
      "LaunchInstanceID: 59FE5431-2D00-4507-97BE-BA013F6D43E0\n",
      "MallocNanoZone: 0\n",
      "OLDPWD: /\n",
      "ORIGINAL_XDG_CURRENT_DESKTOP: undefined\n",
      "PATH: /Users/jamiepan/.pyenv/versions/3.11.3/bin:/Users/jamiepan/.local/bin:/opt/miniconda3/bin:/opt/miniconda3/condabin:/Users/jamiepan/.pyenv/shims:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/X11/bin:/Library/Apple/usr/bin\n",
      "PWD: /\n",
      "PYENV_ROOT: /Users/jamiepan/.pyenv\n",
      "PYENV_SHELL: zsh\n",
      "SECURITYSESSIONID: 186ba\n",
      "SHELL: /bin/zsh\n",
      "SHLVL: 0\n",
      "SSH_AUTH_SOCK: /private/tmp/com.apple.launchd.2dXaTrfVg9/Listeners\n",
      "TMPDIR: /var/folders/86/0xt8cgcs6cs63qjy82mvg7hm0000gn/T/\n",
      "USER: jamiepan\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_CODE_CACHE_PATH: /Users/jamiepan/Library/Application Support/Code/CachedData/5437499feb04f7a586f677b155b039bc2b3669eb\n",
      "VSCODE_CRASH_REPORTER_PROCESS_TYPE: extensionHost\n",
      "VSCODE_CWD: /\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "VSCODE_IPC_HOOK: /Users/jamiepan/Library/Application Support/Code/1.90-main.sock\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en-gb\",\"osLocale\":\"en-au\",\"availableLanguages\":{},\"_languagePackSupport\":true}\n",
      "VSCODE_PID: 1576\n",
      "XPC_FLAGS: 0x0\n",
      "XPC_SERVICE_NAME: application.com.microsoft.VSCode.58615849.58615855\n",
      "_: /Users/jamiepan/Desktop/Visual Studio Code.app/Contents/MacOS/Electron\n",
      "__CFBundleIdentifier: com.microsoft.VSCode\n",
      "__CF_USER_TEXT_ENCODING: 0x1F5:0x0:0xF\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL: 1\n",
      "VSCODE_L10N_BUNDLE_LOCATION: \n",
      "TWITTER_CONSUMER_KEY: Kb9FChkVxNgaiYgwGbVdXKaJ8\n",
      "TWITTER_CONSUMER_SECRET: l2pJl9VsOvCj6MT4Hr93g01Ad4JsO26xyPjv6zJLb8Gg6esUyv\n",
      "TWITTER_ACCESS_TOKEN: 1772073225801105408-tqFHOmYPqyls175uC377KsqX3t4lr3\n",
      "TWITTER_ACCESS_SECRET: l4iOqhhmsmoWH9lGlv5f7u2NFw1PaHCNrsSuLrWLOm2rz\n",
      "OPENAI_API_KEY: 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\n",
      "LLAMA_API: LL-DwhskGz0ai5WDqJfJ2yOzjlT8R1R0ntENPv2g66pVMPhtK6tvsIxKzZUkIj5ZNhG\n",
      "GEMINI_API: AIzaSyAMreykXDPh49daBj7yl2FTrmUEL61FNl4\n",
      "COHERE_API_KEY: S3pGYTHanvAYMRoup25RSP76Uozj01RIeD9fe40E\n",
      "AI21_API_KEY: iZmmQ9h93ZRi1R81TCXQMq66UxD4RNxV\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PYTHONUNBUFFERED: 1\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYTHON_FROZEN_MODULES: on\n",
      "LC_CTYPE: UTF-8\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Print all environment variables to debug\n",
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT experimentation with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross output between models, see if certain model can figure out what prompts are asked by other model and check result\n",
    "\n",
    "Probelm: A lot of model requires paid money. Is it okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validating and attacking them to see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"I would rate the likelihood of this text being AI-generated as **20**. \\n\\nHere's why:\\n\\n* **Common Phrases:** The text uses common phrases like \\\"Great news!\\\" and \\\"Let's keep up the momentum!\\\" which are often found in human-written social media posts.\\n* **Simple Sentence Structure:** The sentences are simple and straightforward, typical of human communication.\\n* **Hashtag Use:** The hashtags are relevant to the content and are a common practice in human-written social media posts.\\n\\nWhile the text is positive and encouraging, it lacks the complexity and nuanced language often seen in AI-generated text. Therefore, it's more likely to be written by a human. \\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 63,\n",
      "        \"candidates_token_count\": 145,\n",
      "        \"total_token_count\": 208\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def detect_ai_generated_gemini(text):\n",
    "    \"\"\"\n",
    "    Detects if a given text is AI-generated using the Gemini model.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to check.\n",
    "    \n",
    "    Returns:\n",
    "    int: Likelihood that the text is AI-generated (0-100).\n",
    "    \"\"\"\n",
    "    prompt = f\"Rate the likelihood that the following text is AI-generated on a scale from 0 to 100: {text}\"\n",
    "    response =model.generate_content([prompt])\n",
    "   \n",
    "    return response\n",
    "\n",
    "answer =detect_ai_generated_gemini(rephrased_tweet_co )\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder for cross validate of all validating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(tweet):\n",
    "    \"\"\"\n",
    "    Cross-validates the rephrased tweets from Gemini and Cohere models to see if they can detect AI-generated content.\n",
    "    \n",
    "    Parameters:\n",
    "    tweet (str): The original tweet.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the original tweet, rephrased tweets, and detection results.\n",
    "    \"\"\"\n",
    "    gemini_rephrased = rephrase_tweet_gemini(tweet)\n",
    "    cohere_rephrased = rephrase_tweet_cohere(tweet)\n",
    "    \n",
    "    gemini_detected = detect_ai_generated_gemini(gemini_rephrased)\n",
    "    cohere_detected = detect_ai_generated_cohere(cohere_rephrased)\n",
    "    \n",
    "    return {\n",
    "        \"original_tweet\": tweet,\n",
    "        \"gemini_rephrased\": gemini_rephrased,\n",
    "        \"gemini_detected\": gemini_detected,\n",
    "        \"cohere_rephrased\": cohere_rephrased,\n",
    "        \"c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Fetch Tweet Text\n",
    "\n",
    "This function retrieves the text of a tweet by its ID using the Twitter API. It handles exceptions by printing an error message.\n",
    "\n",
    "\n",
    "Problem: Tweeter developer account need to be paid to use its recall API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(tweet_id):\n",
    "    \"\"\"Fetch the text of a tweet given its ID.\"\"\"\n",
    "    try:\n",
    "        tweet = twitter_api.get_status(tweet_id, tweet_mode='extended')\n",
    "        return tweet.full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweet {tweet_id}: {e}\")\n",
    "        return None\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
